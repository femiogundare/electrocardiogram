{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import collections\n",
    "import scipy.stats as sst\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "sys.path.append('C:\\\\Users\\\\Dell\\\\Desktop\\\\CV Projects\\\\ecg')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import load\n",
    "import network\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'C:\\\\Users\\\\Dell\\\\Desktop\\\\CV Projects\\\\ecg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(base_dir, 'saved/ecg_experiment_01/090607/0.534-0.954-011-0.430-0.966.hdf5')\n",
    "data_json = os.path.join(base_dir, 'data/val.json')\n",
    "config_file = os.path.join(base_dir, 'config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 852/852 [00:28<00:00, 30.23it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = utilities.load(os.path.dirname(model_path))\n",
    "dataset = load.load_dataset(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A', 1: 'N', 2: 'O', 3: '~'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.int_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = json.load(open(config_file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\n",
    "        'input_shape' : [None, 1],\n",
    "        'num_categories' : len(preprocessor.classes)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network.build_network(**params)\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = model.predict(ecgs, verbose=0)\n",
    "#probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function _make_execution_function.<locals>.distributed_function at 0x0000026676B89E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function _make_execution_function.<locals>.distributed_function at 0x0000026676B89E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function _make_execution_function.<locals>.distributed_function at 0x0000026676B89E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "labels = []\n",
    "for x, y  in zip(*dataset):\n",
    "    x, y = preprocessor.process([x], [y])\n",
    "    probs.append(model.predict(x))\n",
    "    labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(base_dir, 'data/train.json')\n",
    "with open(data_path, 'r') as fid:\n",
    "    train_labels = [json.loads(l)['labels'] for l in fid]\n",
    "counts = collections.Counter(preprocessor.class_to_int[l[0]] for l in train_labels)\n",
    "counts = sorted(counts.most_common(), key=lambda x: x[0])\n",
    "counts = list(zip(*counts))[1]\n",
    "smooth = 500\n",
    "counts = np.array(counts)[None, None, :]\n",
    "total = np.sum(counts) + counts.shape[1]\n",
    "prior = (counts + smooth) / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "ground_truth = []\n",
    "for p, g in zip(probs, labels):\n",
    "    preds.append(sst.mode(np.argmax(p / prior, axis=2).squeeze())[0][0])\n",
    "    ground_truth.append(sst.mode(np.argmax(g, axis=2).squeeze())[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = []\n",
    "#ground_truth = []\n",
    "#for p, g in zip(probs, labels):\n",
    "#    preds.append(sst.mode(np.argmax(p, axis=2).squeeze())[0][0])\n",
    "#    ground_truth.append(sst.mode(np.argmax(g, axis=2).squeeze())[0][0])\n",
    "#    \n",
    "#report = classification_report(\n",
    "#            ground_truth, preds,\n",
    "#            target_names=preprocessor.classes,\n",
    "#            digits=3)\n",
    "#scores = precision_recall_fscore_support(\n",
    "#                    ground_truth,\n",
    "#                    preds,\n",
    "#                    average=None)\n",
    "#print(report)\n",
    "#print(\"Average {:3f}\".format(np.mean(scores[2][:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A      0.723     0.822     0.769        73\n",
      "           N      0.916     0.835     0.873       508\n",
      "           O      0.665     0.759     0.709       228\n",
      "           ~      0.565     0.605     0.584        43\n",
      "\n",
      "    accuracy                          0.802       852\n",
      "   macro avg      0.717     0.755     0.734       852\n",
      "weighted avg      0.815     0.802     0.806       852\n",
      "\n",
      "Average 0.783858\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(\n",
    "            ground_truth, preds,\n",
    "            target_names=preprocessor.classes,\n",
    "            digits=3)\n",
    "scores = metrics.precision_recall_fscore_support(\n",
    "                    ground_truth,\n",
    "                    preds,\n",
    "                    average=None)\n",
    "print(report)\n",
    "print(\"Average {:3f}\".format(np.mean(scores[2][:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    stat_dict = {}\n",
    "    for i in np.unique(y_true):\n",
    "        tp = np.sum(y_true[y_true==i] == y_pred[y_true==i])\n",
    "        fp = np.sum(y_true[y_pred==i] != y_pred[y_pred==i])\n",
    "        fn = np.sum(y_true==i) - tp\n",
    "        tn = np.sum(y_true!=i) - fp\n",
    "        stat_dict[i] = (tp, fp, fn, tn)\n",
    "    return stat_dict\n",
    "\n",
    "\n",
    "def print_results(sd):\n",
    "    print('\\t\\tPrecision  Recall     Specificity     NPV        F1')\n",
    "    tf1 = 0; tot = 0\n",
    "    for k, v in sd.items():\n",
    "        tp, fp, fn, tn = v\n",
    "        precision = tp / float(tp + fp)\n",
    "        recall = tp / float(tp + fn)\n",
    "        specificity = tn / float(tn + fp)\n",
    "        npv = tn / float(tn + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        tf1 += (tp + fn) * f1\n",
    "        tot += (tp + fn)\n",
    "        print('{:>10} {:10.3f} {:10.3f} {:10.3f} {:15.3f} {:10.3f}'.format(\n",
    "            preprocessor.classes[k], precision, recall, specificity, npv, f1))\n",
    "    print('Average F1 {:.3f}'.format(tf1 / float(tot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tPrecision  Recall     Specificity     NPV        F1\n",
      "         A      0.723      0.822      0.970           0.983      0.769\n",
      "         N      0.916      0.835      0.887           0.784      0.873\n",
      "         O      0.665      0.759      0.861           0.907      0.709\n",
      "         ~      0.565      0.605      0.975           0.979      0.584\n",
      "Average F1 0.806\n"
     ]
    }
   ],
   "source": [
    "print_results(stats(ground_truth, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
